      ##comienzo Template
    apiVersion: v1
    kind: Template
    metadata:
      name: kafka-zookeeper-openshift-mvilche
      labels:
        template: kafka-zookeeper-openshift-mvilche
        autor: "Martin_Fabrizzio_Vilche"
      annotations:
        openshift.io/display-name: "kafka-zookeeper-openshift-mvilche"
        iconClass: "icon-github"
        description: >-
          APACHE KAFKA + ZOOKEEPER + KAFKA MANAGER WEB
          Martin Fabrizzio Vilche.
          https://github.com/mvilche.
        openshift.io/provider-display-name: "Martin Fabrizzio Vilche"
        openshift.io/documentation-url: "https://github.com/mvilche/wildfly-s2i.git"
        openshift.io/support-url: "https://github.com/mvilche/wildfly-s2i.git"
    message: >-
      Los servicios iniciarÃ¡n en unos minutos...
      --------------------------------------------------------------
      Grafana

      Usuario admin
      Password admin

      --------------------------------------------------------------
      
      Martin Fabrizzio Vilche           

    objects:


  ############################## ZOOKEPEER


    - apiVersion: v1
      data:
        zoo.cfg: |-
          tickTime=2000
          dataDir=/opt/zookeeper-data
          clientPort=2181
          initLimit=5
          syncLimit=2
          server.1=zookeeper-0.zookeeper.${NAMESPACE}.svc.cluster.local:2888:3888
          server.2=zookeeper-1.zookeeper.${NAMESPACE}.svc.cluster.local:2888:3888
          server.3=zookeeper-2.zookeeper.${NAMESPACE}.svc.cluster.local:2888:3888
          autopurge.snapRetainCount=3
          autopurge.purgeInterval=24
        log4j.properties: |-
          zookeeper.root.logger=INFO, CONSOLE
          zookeeper.console.threshold=INFO
          zookeeper.log.dir=.
          zookeeper.log.file=zookeeper.log
          zookeeper.log.threshold=INFO
          zookeeper.log.maxfilesize=256MB
          zookeeper.log.maxbackupindex=20
          zookeeper.tracelog.dir=${zookeeper.log.dir}
          zookeeper.tracelog.file=zookeeper_trace.log
          log4j.rootLogger=${zookeeper.root.logger}
          log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
          log4j.appender.CONSOLE.Threshold=${zookeeper.console.threshold}
          log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
          log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n
          log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender
          log4j.appender.ROLLINGFILE.Threshold=${zookeeper.log.threshold}
          log4j.appender.ROLLINGFILE.File=${zookeeper.log.dir}/${zookeeper.log.file}
          log4j.appender.ROLLINGFILE.MaxFileSize=${zookeeper.log.maxfilesize}
          log4j.appender.ROLLINGFILE.MaxBackupIndex=${zookeeper.log.maxbackupindex}
          log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout
          log4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n
          log4j.appender.TRACEFILE=org.apache.log4j.FileAppender
          log4j.appender.TRACEFILE.Threshold=TRACE
          log4j.appender.TRACEFILE.File=${zookeeper.tracelog.dir}/${zookeeper.tracelog.file}
          log4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout
          log4j.appender.TRACEFILE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L][%x] - %m%n
      kind: ConfigMap
      metadata:
        name: zookeeper



    - apiVersion: v1
      kind: Service
      metadata:
        name: zookeeper
        labels:
          app: kafka-zookeeper-kafkamanager
      spec:
        ports:
        - port: 2888
          name: server
        - port: 3888
          name: leader-election
        - port: 2181
          name: client
        - port: 8080
          name: http
        - port: 9998
          name: jmx-exporter          
        clusterIP: None
        selector:
          service: zookeeper



    - apiVersion: apps/v1beta1
      kind: StatefulSet
      metadata:
        name: zookeeper
      spec:
        selector:
          matchLabels:
            app: kafka-zookeeper-kafkamanager   
        podManagementPolicy: Parallel
        serviceName: zookeeper
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1
            memory: 2Gi
        replicas: 3
        template:
          metadata:
            annotations:
              alpha.image.policy.openshift.io/resolve-names: '*'
            labels:
              app: kafka-zookeeper-kafkamanager
              service: zookeeper
          spec:
            containers:
            - env:
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                - name: JVMFLAGS
                  value: -javaagent:/opt/zookeeper/jmx_prometheus_javaagent-0.12.0.jar=9998:/opt/prometheus/config.yaml -Dlogging.level=INFO -XX:+PrintFlagsFinal -Djava.awt.headless=true -XX:+UseContainerSupport -XX:MaxRAMPercentage=90.0 -Dfile.encoding=UTF8 -XX:+ExitOnOutOfMemoryError -Djava.net.preferIPv4Stack=true
                - name: JAVA_OPTS
                  value: -Dlogging.level=INFO -XX:+PrintFlagsFinal -Djava.awt.headless=true -XX:+UseContainerSupport -XX:MaxRAMPercentage=90.0 -Dfile.encoding=UTF8 -XX:+ExitOnOutOfMemoryError -Djava.net.preferIPv4Stack=true
              name: zookeeper
              image: "zookeeper:3.5.6"
              imagePullPolicy: Always
              command:
              - bash
              - "-c"
              - |
                export HOST=`hostname -s`
                    if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then
                        NAME=${BASH_REMATCH[1]}
                        ORD=${BASH_REMATCH[2]}
                    else
                        echo "ERROR OBTENIENDO HOSTNAME $HOST"
                        exit 1
                    fi
                    MY_ID=$((ORD+1))
                    if [ ! -f /opt/zookeeper-data/myid ]; then
                    echo $MY_ID >> /opt/zookeeper-data/myid
                    fi
                /opt/zookeeper/bin/zkServer.sh start-foreground /opt/zookeeper/conf/zoo.cfg
              resources:
                requests:
                  cpu: 500m
                  memory: 1Gi
                limits:
                  cpu: 1
                  memory: 2Gi
              ports:
              - containerPort: 2181
                name: client
              - containerPort: 2888
                name: server
              - containerPort: 3888
                name: leader-election
              - containerPort: 8080
                name: http
              - containerPort: 9998
                name: jmx-exporter                
              volumeMounts:
              - name: data
                mountPath: /opt/zookeeper-data
              - name: configmap
                mountPath: /opt/zookeeper/conf
              - name: jmx-exporter-zoo
                mountPath: /opt/prometheus                
              livenessProbe:
                exec:
                  command:
                  - bash
                  - "-c"
                  - |
                    nc -z -v -w5 127.0.0.1 2181
                    if [ "$?" == 0 ]; then
                    exit 0
                    else
                    exit 1
                    fi
                initialDelaySeconds: 90
                timeoutSeconds: 5
              readinessProbe:
                exec:
                  command:
                  - bash
                  - "-c"
                  - |
                    nc -z -v -w5 127.0.0.1 2181
                    if [ "$?" == 0 ]; then
                    exit 0
                    else
                    exit 1
                    fi
                initialDelaySeconds: 30
                timeoutSeconds: 15
            resources:
              requests:
                cpu: 500m
                memory: 128Mi
              limits:
                cpu: 1
                memory: 2048Mi                     
            volumes:
            - name: configmap
              configMap:
                name: zookeeper
            - name: jmx-exporter-zoo
              configMap:
                 name: jmx-exporter-zoo
        volumeClaimTemplates:
        - metadata:
            name: data
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 5Gi


  ############################## FIN ZOOKEEPER



  ##############################KAFKA


    - apiVersion: v1
      kind: Service
      metadata:
        name: kafka
        labels:
          app: kafka-zookeeper-kafkamanager
      spec:
        ports:
        - port: 9092
          name: server1
        - port: 9999
          name: jmx
        - port: 9998
          name: jmx-prometheus          
        clusterIP: None
        selector:
          service: kafka


    - apiVersion: apps/v1beta1
      kind: StatefulSet
      metadata:
        name: kafka
      spec:
        updateStrategy:
          type: RollingUpdate
        selector:
          matchLabels:
            app: kafka-zookeeper-kafkamanager         
        serviceName: kafka      
        replicas: 3
        template:
          metadata:
            annotations:
              alpha.image.policy.openshift.io/resolve-names: '*'
            labels:
              app: kafka-zookeeper-kafkamanager
              service: kafka
          spec:
            initContainers:
            - name: wait-zookeeper
              image: kafka:2.4.1
              command: ['bash', '-c', 'until nc -z -v -w5 zookeeper 2181; do echo waiting for zookeeper; sleep 5; done;']
            containers:
            - env:
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                - name: KAFKA_HEAP_OPTS
                  value: -javaagent:/opt/kafka/jmx_prometheus_javaagent-0.12.0.jar=9998:/opt/prometheus/config.yaml -Dlogging.level=INFO -XX:+PrintFlagsFinal -Djava.awt.headless=true -XX:+UseContainerSupport -XX:MaxRAMPercentage=90.0 -Dfile.encoding=UTF8 -XX:+ExitOnOutOfMemoryError -Djava.net.preferIPv4Stack=true
                - name: JAVA_OPTS
                  value: -Dlogging.level=INFO -XX:+PrintFlagsFinal -Djava.awt.headless=true -XX:+UseContainerSupport -XX:MaxRAMPercentage=90.0 -Dfile.encoding=UTF8 -XX:+ExitOnOutOfMemoryError -Djava.net.preferIPv4Stack=true
                - name: JMX_PORT
                  value: '9999'
              name: kafka
              image: "kafka:2.4.1"
              imagePullPolicy: Always
              command:
              - bash
              - "-c"
              - |
                rm -rf /opt/kafka-storage/.lock
                /opt/checkZoo zookeeper
                /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
                --override listeners=PLAINTEXT://:9092 \
                --override zookeeper.connect=zookeeper-0.zookeeper.$NAMESPACE.svc.cluster.local:2181,zookeeper-1.zookeeper.$NAMESPACE.svc.cluster.local:2181,zookeeper-2.zookeeper.$NAMESPACE.svc.cluster.local:2181 \
                --override log.dirs=/opt/kafka-storage \
                --override auto.create.topics.enable=true \
                --override auto.leader.rebalance.enable=true \
                --override background.threads=10 \
                --override compression.type=producer \
                --override delete.topic.enable=true \
                --override leader.imbalance.check.interval.seconds=300 \
                --override leader.imbalance.per.broker.percentage=10 \
                --override log.flush.interval.messages=9223372036854775807 \
                --override log.flush.offset.checkpoint.interval.ms=60000 \
                --override log.flush.scheduler.interval.ms=9223372036854775807 \
                --override log.retention.bytes=-1 \
                --override log.retention.hours=168 \
                --override log.roll.hours=168 \
                --override log.roll.jitter.hours=0 \
                --override log.segment.bytes=1073741824 \
                --override log.segment.delete.delay.ms=60000 \
                --override message.max.bytes=1000012 \
                --override min.insync.replicas=1 \
                --override num.io.threads=8 \
                --override num.network.threads=3 \
                --override num.recovery.threads.per.data.dir=1 \
                --override num.replica.fetchers=1 \
                --override offset.metadata.max.bytes=4096 \
                --override offsets.commit.required.acks=-1 \
                --override offsets.commit.timeout.ms=5000 \
                --override offsets.load.buffer.size=5242880 \
                --override offsets.retention.check.interval.ms=600000 \
                --override offsets.retention.minutes=143200 \
                --override offsets.topic.compression.codec=0 \
                --override offsets.topic.num.partitions=50 \
                --override offsets.topic.replication.factor=3 \
                --override offsets.topic.segment.bytes=104857600 \
                --override queued.max.requests=500 \
                --override quota.consumer.default=9223372036854775807 \
                --override quota.producer.default=9223372036854775807 \
                --override replica.fetch.min.bytes=1 \
                --override replica.fetch.wait.max.ms=500 \
                --override replica.high.watermark.checkpoint.interval.ms=5000 \
                --override replica.lag.time.max.ms=10000 \
                --override replica.socket.receive.buffer.bytes=65536 \
                --override replica.socket.timeout.ms=30000 \
                --override request.timeout.ms=30000 \
                --override socket.receive.buffer.bytes=102400 \
                --override socket.request.max.bytes=104857600 \
                --override socket.send.buffer.bytes=102400 \
                --override unclean.leader.election.enable=true \
                --override zookeeper.session.timeout.ms=6000 \
                --override zookeeper.set.acl=false \
                --override broker.id.generation.enable=true \
                --override connections.max.idle.ms=600000 \
                --override controlled.shutdown.enable=false \
                --override controlled.shutdown.max.retries=3 \
                --override controlled.shutdown.retry.backoff.ms=5000 \
                --override controller.socket.timeout.ms=30000 \
                --override default.replication.factor=1 \
                --override fetch.purgatory.purge.interval.requests=1000 \
                --override group.max.session.timeout.ms=300000 \
                --override group.min.session.timeout.ms=6000 \
                --override log.cleaner.backoff.ms=15000 \
                --override log.cleaner.dedupe.buffer.size=134217728 \
                --override log.cleaner.delete.retention.ms=86400000 \
                --override log.cleaner.enable=true \
                --override log.cleaner.io.buffer.load.factor=0.9 \
                --override log.cleaner.io.buffer.size=524288 \
                --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \
                --override log.cleaner.min.cleanable.ratio=0.5 \
                --override log.cleaner.min.compaction.lag.ms=0 \
                --override log.cleaner.threads=1 \
                --override log.cleanup.policy=delete \
                --override log.index.interval.bytes=4096 \
                --override log.index.size.max.bytes=10485760 \
                --override log.message.timestamp.difference.max.ms=9223372036854775807 \
                --override log.message.timestamp.type=CreateTime \
                --override log.preallocate=false \
                --override log.retention.check.interval.ms=300000 \
                --override max.connections.per.ip=2147483647 \
                --override num.partitions=1 \
                --override producer.purgatory.purge.interval.requests=1000 \
                --override replica.fetch.backoff.ms=1000 \
                --override replica.fetch.max.bytes=1048576 \
                --override replica.fetch.response.max.bytes=10485760 \
                --override reserved.broker.max.id=1000
              livenessProbe:
                exec:
                  command:
                  - bash
                  - "-c"
                  - |
                    nc -z -v -w5 127.0.0.1 9092
                    if [ "$?" == 0 ]; then
                    exit 0
                    else
                    exit 1
                    fi
                initialDelaySeconds: 90
                timeoutSeconds: 5
              readinessProbe:
                exec:
                  command:
                  - bash
                  - "-c"
                  - |
                    nc -z -v -w5 127.0.0.1 9092
                    if [ "$?" == 0 ]; then
                    exit 0
                    else
                    exit 1
                    fi
                initialDelaySeconds: 90
                timeoutSeconds: 5
              resources:
                requests:
                  cpu: 500m
                  memory: 128Mi
                limits:
                  cpu: 1
                  memory: 1024Mi                
              ports:
              - containerPort: 9092
                name: server1
              - containerPort: 9999
                name: jmx
              - containerPort: 9998
                name: jmx-prom                                           
              volumeMounts:
              - name: data
                mountPath: /opt/kafka-storage
              - name: jmx-exporter-kafka
                mountPath: /opt/prometheus
            volumes:
            - name: jmx-exporter-kafka
              configMap:
                name: jmx-exporter-kafka            
        volumeClaimTemplates:
        - metadata:
            name: data
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 5Gi


  #####################FIN KAFKA


  ###################BUILDCONFIGS

    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
          build: kafka
        name: kafka
      spec:
        failedBuildsHistoryLimit: 5
        nodeSelector: null
        output:
          to:
            kind: ImageStreamTag
            name: 'kafka:2.4.1'
        postCommit: {}
        resources: {}
        runPolicy: Serial
        source:
          contextDir: kafka
          git:
            ref: develop
            uri: 'https://github.com/mvilche/apache-kafka-openshift.git'
          type: Git
        strategy:
          dockerStrategy:
            noCache: true
            forcePull: true
            dockerfilePath: Dockerfile
          type: Docker
        successfulBuildsHistoryLimit: 5
        triggers:
          - type: ConfigChange


    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka
      tag: 2.3.0  
      spec:
        lookupPolicy:
          local: true      
      



    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
          build: zookeeper
        name: zookeeper
      spec:
        failedBuildsHistoryLimit: 5
        nodeSelector: null
        output:
          to:
            kind: ImageStreamTag
            name: 'zookeeper:3.5.6'
        postCommit: {}
        resources: {}
        runPolicy: Serial
        source:
          contextDir: zookeeper
          git:
            ref: develop
            uri: 'https://github.com/mvilche/apache-kafka-openshift.git'
          type: Git
        strategy:
          dockerStrategy:
            noCache: true
            forcePull: true
            dockerfilePath: Dockerfile
          type: Docker
        successfulBuildsHistoryLimit: 5
        triggers:
          - type: ConfigChange


    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: zookeeper
      tag: 3.5.5        
      spec:
        lookupPolicy:
          local: true  




    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
          build: kafka-exporter
        name: kafka-exporter
      spec:
        failedBuildsHistoryLimit: 5
        nodeSelector: null
        output:
          to:
            kind: ImageStreamTag
            name: 'kafka-exporter:latest'
        postCommit: {}
        resources: {}
        runPolicy: Serial
        source:
          contextDir: kafka_exporter
          git:
            ref: develop
            uri: 'https://github.com/mvilche/apache-kafka-openshift.git'
          type: Git
        strategy:
          dockerStrategy:
            noCache: true
            forcePull: true
            dockerfilePath: Dockerfile
          type: Docker
        successfulBuildsHistoryLimit: 5
        triggers:
          - type: ConfigChange


    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-exporter
      tag: latest        
      spec:
        lookupPolicy:
          local: true  



  ########################

  #####kafka exporter
    - apiVersion: v1
      kind: DeploymentConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: "kafka-exporter"
      spec:
        replicas: 1
        revisionHistoryLimit: 10
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: kafka-exporter
        strategy:
          activeDeadlineSeconds: 21600
          resources:
            requests:
              cpu: 500m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 256Mi
          rollingParams:
            intervalSeconds: 1
            maxSurge: 25%
            maxUnavailable: 25%
            timeoutSeconds: 600
            updatePeriodSeconds: 1
          type: Rolling
        template:
          metadata:
            labels:
              app: kafka-zookeeper-kafkamanager
              deploymentconfig: kafka-exporter
            annotations:
              alpha.image.policy.openshift.io/resolve-names: '*'              
          spec:
            initContainers:
              - name: wait-kafka
                image: kafka:2.4.1
                command: ['bash', '-c', 'until nc -z -v -w5 kafka 9092; do echo waiting for kafka; sleep 5; done;']            
            containers:
              - env:
                  - name: NAMESPACE
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: metadata.namespace
                command:
                - sh
                - "-c"
                - |
                  /opt/kafka_exporter --kafka.server=kafka-0.kafka.$NAMESPACE.svc.cluster.local:9092 \
                  --kafka.server=kafka-1.kafka.$NAMESPACE.svc.cluster.local:9092 \
                  --kafka.server=kafka-2.kafka.$NAMESPACE.svc.cluster.local:9092                        
                image: "kafka-exporter:latest"
                imagePullPolicy: Always
                name: kafka-exporter
                livenessProbe:
                  failureThreshold: 3
                  initialDelaySeconds: 60
                  periodSeconds: 20
                  successThreshold: 1
                  httpGet:
                    port: 9308
                    path: /metrics
                  timeoutSeconds: 15
                readinessProbe:
                  failureThreshold: 3
                  initialDelaySeconds: 60
                  periodSeconds: 20
                  successThreshold: 1
                  httpGet:
                    port: 9308
                    path: /metrics
                  timeoutSeconds: 15
                resources:
                  requests:
                    cpu: 500m
                    memory: 128Mi
                  limits:
                    cpu: 500m
                    memory: 256Mi
                ports:
                  - containerPort: 9308
                    protocol: TCP
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: Always
            schedulerName: default-scheduler
            terminationGracePeriodSeconds: 30
        test: false
        triggers:
          - type: ConfigChange
          - imageChangeParams:
              automatic: true
              containerNames:
                - kafka-exporter
              from:
                kind: ImageStreamTag
                name: "kafka-exporter:latest"
            type: ImageChange


    - apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-exporter
      spec:
        ports:
          - name: http
            port: 9308
            protocol: TCP
            targetPort: 9308
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: kafka-exporter
        sessionAffinity: None
        type: ClusterIP


    - apiVersion: v1
      kind: Route
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-exporter-http
      spec:
        port:
          targetPort: http
        to:
          kind: Service
          name: kafka-exporter
          weight: 100
        wildcardPolicy: None

  ####


  ##################### KAFKA MANAGER


    - apiVersion: v1
      kind: DeploymentConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: "kafka-manager"
      spec:
        replicas: 1
        revisionHistoryLimit: 10
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: kafka-manager
        strategy:
          activeDeadlineSeconds: 21600
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: 1
              memory: 2Gi
          rollingParams:
            intervalSeconds: 1
            maxSurge: 25%
            maxUnavailable: 25%
            timeoutSeconds: 600
            updatePeriodSeconds: 1
          type: Rolling
        template:
          metadata:
            labels:
              app: kafka-zookeeper-kafkamanager
              deploymentconfig: kafka-manager
            annotations:
              alpha.image.policy.openshift.io/resolve-names: '*'              
          spec:
            containers:
              - initContainers:
                - name: wait-zookeeper
                  image: kafka:2.4.1
                  command: ['bash', '-c', 'until nc -z -v -w5 zookeeper-0.zookeeper.${NAMESPACE}.svc.cluster.local 2181; do echo waiting for zookeeper; sleep 5; done;']
                env:
                  - name: NAMESPACE
                    valueFrom:
                      fieldRef:
                        apiVersion: v1
                        fieldPath: metadata.namespace
                  - name: JAVA_OPTS
                    value: -Dlogging.level=INFO -Djava.awt.headless=true -XX:MaxRAMFraction=1 -Dfile.encoding=UTF8 -XX:+ExitOnOutOfMemoryError -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.net.preferIPv4Stack=true
                image: "kafka-manager:latest"
                imagePullPolicy: Always
                name: kafka-manager
                livenessProbe:
                  failureThreshold: 3
                  initialDelaySeconds: 60
                  periodSeconds: 20
                  successThreshold: 1
                  httpGet:
                    port: 9000
                    path: /
                  timeoutSeconds: 15
                readinessProbe:
                  failureThreshold: 3
                  initialDelaySeconds: 60
                  periodSeconds: 20
                  successThreshold: 1
                  httpGet:
                    port: 9000
                    path: /
                  timeoutSeconds: 15
                resources:
                  requests:
                    cpu: 500m
                    memory: 1Gi
                  limits:
                    cpu: 1
                    memory: 1Gi
                ports:
                  - containerPort: 9000
                    protocol: TCP
                lifecycle:
                  postStart:
                    exec:
                      command:
                      - bash
                      - "-c"
                      - |
                        OK=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:9000/)
                        while [  $OK != 200 ]; do
                            echo "aun no activo"
                            let OK=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:9000/)
                        done
                        curl -d "name=openshift&zkHosts=zookeeper-0.zookeeper.${NAMESPACE}.svc.cluster.local%3A2181%2Czookeeper-1.zookeeper.${NAMESPACE}.svc.cluster.local%3A2181%2Czookeeper-2.zookeeper.${NAMESPACE}.svc.cluster.local%3A2181&kafkaVersion=2.2.0&jmxEnabled=true&jmxUser=&jmxPass=&tuning.brokerViewUpdatePeriodSeconds=30&tuning.clusterManagerThreadPoolSize=2&tuning.clusterManagerThreadPoolQueueSize=100&tuning.kafkaCommandThreadPoolSize=2&tuning.kafkaCommandThreadPoolQueueSize=100&tuning.logkafkaCommandThreadPoolSize=2&tuning.logkafkaCommandThreadPoolQueueSize=100&tuning.logkafkaUpdatePeriodSeconds=30&tuning.partitionOffsetCacheTimeoutSecs=5&tuning.brokerViewThreadPoolSize=3&tuning.brokerViewThreadPoolQueueSize=1000&tuning.offsetCacheThreadPoolSize=3&tuning.offsetCacheThreadPoolQueueSize=1000&tuning.kafkaAdminClientThreadPoolSize=3&tuning.kafkaAdminClientThreadPoolQueueSize=1000&tuning.kafkaManagedOffsetMetadataCheckMillis=30000&tuning.kafkaManagedOffsetGroupCacheSize=1000000&tuning.kafkaManagedOffsetGroupExpireDays=7&securityProtocol=PLAINTEXT&saslMechanism=DEFAULT&jaasConfig=" -X POST http://localhost:9000/clusters || exit 0
                volumeMounts:
                - name: kafka-manager
                  mountPath: /opt/kafka-manager-2.0.0.2/conf
                  terminationMessagePath: /dev/termination-log
                  terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            volumes:
            - name: kafka-manager
              configMap:
                name: kafka-manager
            restartPolicy: Always
            schedulerName: default-scheduler
            terminationGracePeriodSeconds: 30
        test: false
        triggers:
          - type: ConfigChange
          - imageChangeParams:
              automatic: true
              containerNames:
                - kafka-manager
              from:
                kind: ImageStreamTag
                name: "kafka-manager:latest"
            type: ImageChange


    - apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-manager
      spec:
        ports:
          - name: http
            port: 9000
            protocol: TCP
            targetPort: 9000
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: kafka-manager
        sessionAffinity: None
        type: ClusterIP


    - apiVersion: v1
      kind: Route
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-manager-http
      spec:
        port:
          targetPort: http
        to:
          kind: Service
          name: kafka-manager
          weight: 100
        wildcardPolicy: None


    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
          build: kafka-manager-sbt
        name: kafka-manager-sbt
      spec:
        failedBuildsHistoryLimit: 5
        nodeSelector: null
        output:
          to:
            kind: ImageStreamTag
            name: 'kafka-manager-sbt:latest'
        postCommit: {}
        resources: {}
        runPolicy: Serial
        source:
          contextDir: kafka-manager
          git:
            ref: develop
            uri: 'https://github.com/mvilche/apache-kafka-openshift.git'
          type: Git
        strategy:
          dockerStrategy:
            noCache: true
            forcePull: true
            dockerfilePath: Dockerfile
            
          type: Docker
        successfulBuildsHistoryLimit: 5
        triggers:
          - type: ConfigChange

    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-manager-sbt
      spec:
        lookupPolicy:
          local: true  


    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-manager
      spec:
        completionDeadlineSeconds: 1800
        output:
          to:
            kind: ImageStreamTag
            name: kafka-manager:latest
        source:
          type: Dockerfile
          dockerfile: |-
                  FROM alpine:3.10
                  USER root
                  RUN apk add --update --no-cache shadow busybox-suid openjdk8 bash curl && mkdir -p /opt
                  COPY kafka-manager-2.0.0.2 /opt/kafka-manager-2.0.0.2
                  RUN adduser -u 1001 -D -h /opt/kafka-manager-2.0.0.2 app && usermod -aG 0 app && \
                  chown 1001:0 -R /opt && \
                  chgrp -R 0 /opt && \
                  chmod +x /opt/kafka-manager-2.0.0.2/bin/* && \
                  chmod -R g=u /opt
                  USER 1001
                  ENV HOME /opt/kafka-manager-2.0.0.2
                  CMD ["/opt/kafka-manager-2.0.0.2/bin/kafka-manager", "-Dconfig.file=/opt/kafka-manager-2.0.0.2/conf/application.conf", "-Dapplication.home=/opt/kafka-manager-2.0.0.2"]
          images:
          - from:
              kind: ImageStreamTag
              name: kafka-manager-sbt:latest
            paths:
            - sourcePath: /opt/kafka-manager/target/universal/kafka-manager-2.0.0.2
              destinationDir: "."
        strategy:
          type: Docker
        triggers:
        - type: "imageChange"
          imageChange:
            from:
              kind: "ImageStreamTag"
              name: "kafka-manager-sbt:latest"


    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: kafka-manager
      spec:
        lookupPolicy:
          local: true  



    - apiVersion: v1
      data:
        routes: |-
          # Copyright 2015 Yahoo Inc. Licensed under the Apache License, Version 2.0
          # See accompanying LICENSE file.
          #
          # Routes
          # This file defines all application routes (Higher priority routes first)
          # ~~~~
          # Home page
          GET    /                                                    controllers.Application.index
          GET    /clusters/:c                                         controllers.Cluster.cluster(c:String)
          GET    /clusters/:c/topics                                  controllers.Topic.topics(c:String)
          GET    /clusters/:c/topics/addPartitions                    controllers.Topic.addPartitionsToMultipleTopics(c:String)
          POST   /clusters/:c/topics/addPartitions                    controllers.Topic.handleAddPartitionsToMultipleTopics(c:String)
          GET    /clusters/:c/topics/:t                               controllers.Topic.topic(c:String, t:String, force:Boolean ?= false)
          GET    /clusters/:c/logkafkas                               controllers.Logkafka.logkafkas(c:String)
          GET    /clusters/:c/logkafkas/:h/:l                         controllers.Logkafka.logkafka(c:String, h:String, l:String)
          GET    /clusters/:c/brokers                                 controllers.Cluster.brokers(c: String)
          GET    /clusters/:c/brokers/:b                              controllers.Cluster.broker(c: String, b:Int)
          GET    /clusters/:c/consumers                               controllers.Consumer.consumers(c: String)
          GET    /clusters/:c/consumers/:g/type/:ct                   controllers.Consumer.consumer(c: String, g:String, ct: String)
          GET    /clusters/:c/consumers/:g/topic/:t/type/:ct          controllers.Consumer.consumerAndTopic(c: String, g:String, t:String, ct: String)
          GET    /clusters/:c/leader                                  controllers.PreferredReplicaElection.preferredReplicaElection(c:String)
          POST   /clusters/:c/leader                                  controllers.PreferredReplicaElection.handleRunElection(c:String)
          GET    /clusters/:c/assignment                              controllers.ReassignPartitions.reassignPartitions(c:String)
          POST   /clusters/:c/assignment                              controllers.ReassignPartitions.handleOperation(c:String,t:String)
          GET    /clusters/:c/assignment/confirm                      controllers.ReassignPartitions.confirmAssignment(c:String,t:String)
          POST   /clusters/:c/assignment/generate                     controllers.ReassignPartitions.handleGenerateAssignment(c:String,t:String)
          GET    /clusters/:c/assignments/confirm                     controllers.ReassignPartitions.confirmMultipleAssignments(c:String)
          POST   /clusters/:c/assignments/generate                    controllers.ReassignPartitions.handleGenerateMultipleAssignments(c:String)
          #GET   /clusters/:c/assignments/manual                      controllers.ReassignPartitions.manualMultipleAssignments(c:String)
          #POST  /clusters/:c/assignments/manual                      controllers.ReassignPartitions.handleManualAssignment(c:String)
          GET    /clusters/:c/assignments/run                         controllers.ReassignPartitions.runMultipleAssignments(c:String)
          POST   /clusters/:c/assignments/run                         controllers.ReassignPartitions.handleRunMultipleAssignments(c:String)
          GET    /addCluster                                          controllers.Cluster.addCluster
          GET    /updateCluster                                       controllers.Cluster.updateCluster(c: String)
          POST   /clusters                                            controllers.Cluster.handleAddCluster
          POST   /clusters/:c                                         controllers.Cluster.handleUpdateCluster(c:String)
          GET    /clusters/:c/createTopic                             controllers.Topic.createTopic(c:String)
          POST   /clusters/:c/topics/create                           controllers.Topic.handleCreateTopic(c:String)
          GET    /clusters/:c/topics/:t/confirm_delete                controllers.Topic.confirmDeleteTopic(c:String,t:String)
          POST   /clusters/:c/topics/delete                           controllers.Topic.handleDeleteTopic(c:String,t:String)
          GET    /clusters/:c/topics/:t/addPartitions                 controllers.Topic.addPartitions(c:String,t:String)
          POST   /clusters/:c/topics/:t/addPartitions                 controllers.Topic.handleAddPartitions(c:String,t:String)
          GET    /clusters/:c/topics/:t/updateConfig                  controllers.Topic.updateConfig(c:String,t:String)
          POST   /clusters/:c/topics/:t/updateConfig                  controllers.Topic.handleUpdateConfig(c:String,t: String)
          GET    /clusters/:c/topics/:t/assignments/manual            controllers.ReassignPartitions.manualAssignments(c:String, t:String)
          POST   /clusters/:c/topics/:t/assignments/manual            controllers.ReassignPartitions.handleManualAssignment(c:String, t: String)
          GET    /clusters/:c/createLogkafka                          controllers.Logkafka.createLogkafka(c:String)
          POST   /clusters/:c/logkafkas/create                        controllers.Logkafka.handleCreateLogkafka(c:String)
          POST   /clusters/:c/logkafkas/delete                        controllers.Logkafka.handleDeleteLogkafka(c:String, h:String, l:String)
          GET    /clusters/:c/logkafkas/:h/:l/updateConfig            controllers.Logkafka.updateConfig(c:String, h:String, l:String)
          POST   /clusters/:c/logkafkas/:h/:l/updateConfig            controllers.Logkafka.handleUpdateConfig(c:String, h:String, l:String)
          POST   /clusters/:c/logkafkas/:h/:l/disableConfig           controllers.Logkafka.handleDisableConfig(c:String, h:String, l:String)
          POST   /clusters/:c/logkafkas/:h/:l/enableConfig            controllers.Logkafka.handleEnableConfig(c:String, h:String, l:String)
          GET    /api/status/:c/brokers                               controllers.api.KafkaStateCheck.brokers(c:String)
          GET    /api/status/:c/brokers/extended                      controllers.api.KafkaStateCheck.brokersExtended(c:String)
          GET    /api/status/:c/topics                                controllers.api.KafkaStateCheck.topics(c:String)
          GET    /api/status/:c/topicIdentities                       controllers.api.KafkaStateCheck.topicIdentities(c:String)
          GET    /api/status/clusters                                 controllers.api.KafkaStateCheck.clusters
          GET    /api/status/:c/:t/underReplicatedPartitions          controllers.api.KafkaStateCheck.underReplicatedPartitions(c:String,t:String)
          GET    /api/status/:c/:t/unavailablePartitions              controllers.api.KafkaStateCheck.unavailablePartitions(c:String,t:String)
          GET    /api/status/:cluster/:consumer/:topic/:consumerType/topicSummary   controllers.api.KafkaStateCheck.topicSummaryAction(cluster:String, consumer:String, topic:String, consumerType:String)
          GET    /api/status/:cluster/:consumer/:consumerType/groupSummary          controllers.api.KafkaStateCheck.groupSummaryAction(cluster:String, consumer:String, consumerType:String)
          GET    /api/status/:cluster/consumersSummary                controllers.api.KafkaStateCheck.consumersSummaryAction(cluster:String)
          # Versioned Assets
          GET    /vassets/*file                                       controllers.Assets.versioned(path="/public", file: Asset)
          # Unversioned Assets
          GET    /assets/*file                                        controllers.Assets.at(path="/public", file)
          # Ping / Health Check
          GET    /api/health                                          controllers.ApiHealth.ping
        logger.xml: |-
          <configuration>
              <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
                  <file>${application.home}/logs/application.log</file>
                  <encoder>
                    <pattern>%date - [%level] - from %logger in %thread %n%message%n%xException%n</pattern>
                  </encoder>
                  <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
                      <fileNamePattern>${application.home}/logs/application.%d{yyyy-MM-dd}.log</fileNamePattern>
                      <maxHistory>5</maxHistory>
                      <totalSizeCap>5GB</totalSizeCap>
                  </rollingPolicy>
              </appender>
              <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
                  <encoder>
                      <pattern>%date - [%level] %logger{15} - %message%n%xException{10}</pattern>
                  </encoder>
              </appender>
              <appender name="ASYNCFILE" class="ch.qos.logback.classic.AsyncAppender">
                  <appender-ref ref="FILE" />
              </appender>
              <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
                  <appender-ref ref="STDOUT" />
              </appender>
              <logger name="play" level="INFO" />
              <logger name="application" level="INFO" />
              <logger name="kafka.manager" level="INFO" />
              <!-- Off these ones as they are annoying, and anyway we manage configuration ourself -->
              <logger name="com.avaje.ebean.config.PropertyMapLoader" level="OFF" />
              <logger name="com.avaje.ebeaninternal.server.core.XmlConfigLoader" level="OFF" />
              <logger name="com.avaje.ebeaninternal.server.lib.BackgroundThread" level="OFF" />
              <logger name="com.gargoylesoftware.htmlunit.javascript" level="OFF" />
              <logger name="org.apache.zookeeper" level="INFO"/>
              <root level="WARN">
                  <appender-ref ref="ASYNCFILE" />
                  <appender-ref ref="ASYNCSTDOUT" />
              </root>
          </configuration>
          /kafka-manager/target/universal/kafka-manager-2.0.0.2/conf # ^C
          /kafka-manager/target/universal/kafka-manager-2.0.0.2/conf # ls
          application.conf     consumer.properties  logback.xml          logger.xml           routes
          /kafka-manager/target/universal/kafka-manager-2.0.0.2/conf # cat logger.xml
          <configuration>
            <conversionRule conversionWord="coloredLevel" converterClass="play.api.Logger$ColoredLevel" />
            <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
              <file>${application.home}/logs/application.log</file>
              <encoder>
                <pattern>%date - [%level] - from %logger in %thread %n%message%n%xException%n</pattern>
              </encoder>
              <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
                <fileNamePattern>${application.home}/logs/application.%d{yyyy-MM-dd}.log</fileNamePattern>
                <maxHistory>5</maxHistory>
                <totalSizeCap>5GB</totalSizeCap>
              </rollingPolicy>
            </appender>
            <logger name="play" level="INFO" />
            <logger name="application" level="DEBUG" />
            <!-- Off these ones as they are annoying, and anyway we manage configuration ourself -->
            <logger name="com.avaje.ebean.config.PropertyMapLoader" level="OFF" />
            <logger name="com.avaje.ebeaninternal.server.core.XmlConfigLoader" level="OFF" />
            <logger name="com.avaje.ebeaninternal.server.lib.BackgroundThread" level="OFF" />
            <logger name="com.gargoylesoftware.htmlunit.javascript" level="OFF" />
            <logger name="org.apache.zookeeper" level="INFO"/>
            <logger name="akka" level="INFO" />
            <logger name="kafka" level="INFO" />
            <root level="INFO">
              <appender-ref ref="FILE" />
            </root>
          </configuration>
        logback.xml: |-
          <configuration>
              <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
                  <file>${application.home}/logs/application.log</file>
                  <encoder>
                    <pattern>%date - [%level] - from %logger in %thread %n%message%n%xException%n</pattern>
                  </encoder>
                  <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
                      <fileNamePattern>${application.home}/logs/application.%d{yyyy-MM-dd}.log</fileNamePattern>
                      <maxHistory>5</maxHistory>
                      <totalSizeCap>5GB</totalSizeCap>
                  </rollingPolicy>
              </appender>
              <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
                  <encoder>
                      <pattern>%date - [%level] %logger{15} - %message%n%xException{10}</pattern>
                  </encoder>
              </appender>
              <appender name="ASYNCFILE" class="ch.qos.logback.classic.AsyncAppender">
                  <appender-ref ref="FILE" />
              </appender>
              <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
                  <appender-ref ref="STDOUT" />
              </appender>
              <logger name="play" level="INFO" />
              <logger name="application" level="INFO" />
              <logger name="kafka.manager" level="INFO" />
              <!-- Off these ones as they are annoying, and anyway we manage configuration ourself -->
              <logger name="com.avaje.ebean.config.PropertyMapLoader" level="OFF" />
              <logger name="com.avaje.ebeaninternal.server.core.XmlConfigLoader" level="OFF" />
              <logger name="com.avaje.ebeaninternal.server.lib.BackgroundThread" level="OFF" />
              <logger name="com.gargoylesoftware.htmlunit.javascript" level="OFF" />
              <logger name="org.apache.zookeeper" level="INFO"/>
              <root level="WARN">
                  <appender-ref ref="ASYNCFILE" />
                  <appender-ref ref="ASYNCSTDOUT" />
              </root>
          </configuration>
        consumer.properties: |-
          security.protocol=PLAINTEXT
          key.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer
          value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer
        application.conf: |-
          play.crypto.secret="^<csmm5Fx4d=r2HEX8pelM3iBkFVv?k[mc;IZE<_Qoq8EkX_/7@Zt6dP05Pzea3U"
          play.crypto.secret=${?APPLICATION_SECRET}
          play.http.session.maxAge="1h"
          play.i18n.langs=["en"]
          play.http.requestHandler = "play.http.DefaultHttpRequestHandler"
          play.http.context = "/"
          play.application.loader=loader.KafkaManagerLoader
          kafka-manager.zkhosts="zookeeper-0.zookeeper.${NAMESPACE}.svc.cluster.local:2181,zookeeper-1.zookeeper.${NAMESPACE}.svc.cluster.local:2181,zookeeper-2.zookeeper.${NAMESPACE}.svc.cluster.local:2181"
          kafka-manager.zkhosts=${?ZK_HOSTS}
          pinned-dispatcher.type="PinnedDispatcher"
          pinned-dispatcher.executor="thread-pool-executor"
          application.features=["KMClusterManagerFeature","KMTopicManagerFeature","KMPreferredReplicaElectionFeature","KMReassignPartitionsFeature"]
          akka {
            loggers = ["akka.event.slf4j.Slf4jLogger"]
            loglevel = "INFO"
          }
          akka.logger-startup-timeout = 60s
          basicAuthentication.enabled=false
          basicAuthentication.enabled=${?KAFKA_MANAGER_AUTH_ENABLED}
          basicAuthentication.ldap.enabled=false
          basicAuthentication.ldap.enabled=${?KAFKA_MANAGER_LDAP_ENABLED}
          basicAuthentication.ldap.server=""
          basicAuthentication.ldap.server=${?KAFKA_MANAGER_LDAP_SERVER}
          basicAuthentication.ldap.port=389
          basicAuthentication.ldap.port=${?KAFKA_MANAGER_LDAP_PORT}
          basicAuthentication.ldap.username=""
          basicAuthentication.ldap.username=${?KAFKA_MANAGER_LDAP_USERNAME}
          basicAuthentication.ldap.password=""
          basicAuthentication.ldap.password=${?KAFKA_MANAGER_LDAP_PASSWORD}
          basicAuthentication.ldap.search-base-dn=""
          basicAuthentication.ldap.search-base-dn=${?KAFKA_MANAGER_LDAP_SEARCH_BASE_DN}
          basicAuthentication.ldap.search-filter="(uid=$capturedLogin$)"
          basicAuthentication.ldap.search-filter=${?KAFKA_MANAGER_LDAP_SEARCH_FILTER}
          basicAuthentication.ldap.connection-pool-size=10
          basicAuthentication.ldap.connection-pool-size=${?KAFKA_MANAGER_LDAP_CONNECTION_POOL_SIZE}
          basicAuthentication.ldap.ssl=false
          basicAuthentication.ldap.ssl=${?KAFKA_MANAGER_LDAP_SSL}
          basicAuthentication.username="admin"
          basicAuthentication.username=${?KAFKA_MANAGER_USERNAME}
          basicAuthentication.password="password"
          basicAuthentication.password=${?KAFKA_MANAGER_PASSWORD}
          basicAuthentication.realm="Kafka-Manager"
          basicAuthentication.excluded=["/api/health"] # ping the health of your instance without authentification
          kafka-manager.consumer.properties.file=${?CONSUMER_PROPERTIES_FILE}
      kind: ConfigMap
      metadata:
        name: kafka-manager


    - apiVersion: v1
      data:
        config.yaml: |-
          # Everything has defaults so you only need to uncomment things you want to
          lowercaseOutputLabelNames: true
          lowercaseOutputName: true
          whitelistObjectNames: ["java.lang:type=OperatingSystem"]
          blacklistObjectNames: []
          rules:
            - pattern: 'java.lang<type=OperatingSystem><>(committed_virtual_memory|free_physical_memory|free_swap_space|total_physical_memory|total_swap_space)_size:'
              name: os_$1_bytes
              type: GAUGE
              attrNameSnakeCase: true
            - pattern: 'java.lang<type=OperatingSystem><>((?!process_cpu_time)\w+):'
              name: os_$1
              type: GAUGE
              attrNameSnakeCase: true
            # replicated Zookeeper
            - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+)><>(\\w+)"
              name: "zookeeper_$2"
            - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+)><>(\\w+)"
              name: "zookeeper_$3"
              labels:
                replicaId: "$2"
            - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+)><>(\\w+)"
              name: "zookeeper_$4"
              labels:
                replicaId: "$2"
                memberType: "$3"
            - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+), name3=(\\w+)><>(\\w+)"
              name: "zookeeper_$4_$5"
              labels:
                replicaId: "$2"
                memberType: "$3"
            # standalone Zookeeper
            - pattern: "org.apache.ZooKeeperService<name0=StandaloneServer_port(\\d+)><>(\\w+)"
              name: "zookeeper_$2"
            - pattern: "org.apache.ZooKeeperService<name0=StandaloneServer_port(\\d+), name1=InMemoryDataTree><>(\\w+)"
              name: "zookeeper_$2"              
      kind: ConfigMap
      metadata:
        name: jmx-exporter-zoo        


    - apiVersion: v1
      data:
        config.yaml: |-
          # Everything has defaults so you only need to uncomment things you want to
          lowercaseOutputName: true
          whitelistObjectNames: ["java.lang:type=OperatingSystem"]
          blacklistObjectNames: []
          rules:
            - pattern: 'java.lang<type=OperatingSystem><>(committed_virtual_memory|free_physical_memory|free_swap_space|total_physical_memory|total_swap_space)_size:'
              name: os_$1_bytes
              type: GAUGE
              attrNameSnakeCase: true
            - pattern: 'java.lang<type=OperatingSystem><>((?!process_cpu_time)\w+):'
              name: os_$1
              type: GAUGE
              attrNameSnakeCase: true
          # Special cases and very specific rules
          - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value
            name: kafka_server_$1_$2
            type: GAUGE
            labels:
              clientId: "$3"
              topic: "$4"
              partition: "$5"
          - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>Value
            name: kafka_server_$1_$2
            type: GAUGE
            labels:
              clientId: "$3"
              broker: "$4:$5"
          - pattern : kafka.coordinator.(\w+)<type=(.+), name=(.+)><>Value
            name: kafka_coordinator_$1_$2_$3
            type: GAUGE

          # Generic per-second counters with 0-2 key/value pairs
          - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)><>Count
            name: kafka_$1_$2_$3_total
            type: COUNTER
            labels:
              "$4": "$5"
              "$6": "$7"
          - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+)><>Count
            name: kafka_$1_$2_$3_total
            type: COUNTER
            labels:
              "$4": "$5"
          - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
            name: kafka_$1_$2_$3_total
            type: COUNTER

          - pattern: kafka.server<type=(.+), client-id=(.+)><>([a-z-]+)
            name: kafka_server_quota_$3
            type: GAUGE
            labels:
              resource: "$1"
              clientId: "$2"

          - pattern: kafka.server<type=(.+), user=(.+), client-id=(.+)><>([a-z-]+)
            name: kafka_server_quota_$4
            type: GAUGE
            labels:
              resource: "$1"
              user: "$2"
              clientId: "$3"

          # Generic gauges with 0-2 key/value pairs
          - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Value
            name: kafka_$1_$2_$3
            type: GAUGE
            labels:
              "$4": "$5"
              "$6": "$7"
          - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Value
            name: kafka_$1_$2_$3
            type: GAUGE
            labels:
              "$4": "$5"
          - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Value
            name: kafka_$1_$2_$3
            type: GAUGE

          # Emulate Prometheus 'Summary' metrics for the exported 'Histogram's.
          #
          # Note that these are missing the '_sum' metric!
          - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Count
            name: kafka_$1_$2_$3_count
            type: COUNTER
            labels:
              "$4": "$5"
              "$6": "$7"
          - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)><>(\d+)thPercentile
            name: kafka_$1_$2_$3
            type: GAUGE
            labels:
              "$4": "$5"
              "$6": "$7"
              quantile: "0.$8"
          - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Count
            name: kafka_$1_$2_$3_count
            type: COUNTER
            labels:
              "$4": "$5"
          - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*)><>(\d+)thPercentile
            name: kafka_$1_$2_$3
            type: GAUGE
            labels:
              "$4": "$5"
              quantile: "0.$6"
          - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Count
            name: kafka_$1_$2_$3_count
            type: COUNTER
          - pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
            name: kafka_$1_$2_$3
            type: GAUGE
            labels:
              quantile: "0.$4"
      kind: ConfigMap
      metadata:
        name: jmx-exporter-kafka





    - apiVersion: v1
      data:
        grafana.ini: |-
          # Everything has defaults so you only need to uncomment things you want to
          # change

          # possible values : production, development
          app_mode = production

          # instance name, defaults to HOSTNAME environment variable value or hostname if HOSTNAME var is empty
          instance_name = ${HOSTNAME}

          #################################### Paths ####################################
          [paths]
          # Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)
          data = /opt/grafana-data/data

          # Temporary files in `data` directory older than given duration will be removed
          ;temp_data_lifetime = 24h

          # Directory where grafana can store logs
          ;logs = /var/log/grafana

          # Directory where grafana will automatically scan and look for plugins
          plugins = /opt/grafana-data/plugins

          # folder that contains provisioning config files that grafana will apply on startup and while running.
          provisioning = /opt/grafana/conf/provisioning

          #################################### Server ####################################
          [server]
          # Protocol (http, https, h2, socket)
          protocol = http

          # The ip address to bind to, empty will bind to all interfaces
          ;http_addr =

          # The http port  to use
          http_port = 3000

          # The public facing domain name used to access grafana from a browser
          ;domain = localhost

          # Redirect to correct domain if host header does not match domain
          # Prevents DNS rebinding attacks
          ;enforce_domain = false

          # The full public facing url you use in browser, used for redirects and emails
          # If you use reverse proxy and sub path specify full url (with sub path)
          ;root_url = http://localhost:3000

          # Serve Grafana from subpath specified in `root_url` setting. By default it is set to `false` for compatibility reasons.
          ;serve_from_sub_path = false

          # Log web requests
          ;router_logging = false

          # the path relative working path
          ;static_root_path = public

          # enable gzip
          ;enable_gzip = false

          # https certs & key file
          ;cert_file =
          ;cert_key =

          # Unix socket path
          ;socket =

          #################################### Database ####################################
          [database]
          # You can configure the database connection by specifying type, host, name, user and password
          # as separate properties or as on string using the url properties.

          # Either "mysql", "postgres" or "sqlite3", it's your choice
          ;type = sqlite3
          ;host = 127.0.0.1:3306
          ;name = grafana
          ;user = root
          # If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""
          ;password =

          # Use either URL or the previous fields to configure the database
          # Example: mysql://user:secret@host:port/database
          ;url =

          # For "postgres" only, either "disable", "require" or "verify-full"
          ;ssl_mode = disable

          # For "sqlite3" only, path relative to data_path setting
          ;path = grafana.db

          # Max idle conn setting default is 2
          ;max_idle_conn = 2

          # Max conn setting default is 0 (mean not set)
          ;max_open_conn =

          # Connection Max Lifetime default is 14400 (means 14400 seconds or 4 hours)
          ;conn_max_lifetime = 14400

          # Set to true to log the sql calls and execution times.
          ;log_queries =

          # For "sqlite3" only. cache mode setting used for connecting to the database. (private, shared)
          ;cache_mode = private

          #################################### Cache server #############################
          [remote_cache]
          # Either "redis", "memcached" or "database" default is "database"
          ;type = database

          # cache connectionstring options
          # database: will use Grafana primary database.
          # redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=0,ssl=false`. Only addr is required. ssl may be 'true', 'false', or 'insecure'.
          # memcache: 127.0.0.1:11211
          ;connstr =

          #################################### Data proxy ###########################
          [dataproxy]

          # This enables data proxy logging, default is false
          ;logging = false

          # How long the data proxy should wait before timing out default is 30 (seconds)
          ;timeout = 30

          # If enabled and user is not anonymous, data proxy will add X-Grafana-User header with username into the request, default is false.
          ;send_user_header = false

          #################################### Analytics ####################################
          [analytics]
          # Server reporting, sends usage counters to stats.grafana.org every 24 hours.
          # No ip addresses are being tracked, only simple counters to track
          # running instances, dashboard and error counts. It is very helpful to us.
          # Change this option to false to disable reporting.
          ;reporting_enabled = true

          # Set to false to disable all checks to https://grafana.net
          # for new vesions (grafana itself and plugins), check is used
          # in some UI views to notify that grafana or plugin update exists
          # This option does not cause any auto updates, nor send any information
          # only a GET request to http://grafana.com to get latest versions
          ;check_for_updates = true

          # Google Analytics universal tracking code, only enabled if you specify an id here
          ;google_analytics_ua_id =

          # Google Tag Manager ID, only enabled if you specify an id here
          ;google_tag_manager_id =

          #################################### Security ####################################
          [security]
          # default admin user, created on startup
          ;admin_user = admin

          # default admin password, can be changed before first start of grafana,  or in profile settings
          ;admin_password = admin

          # used for signing
          ;secret_key = SW2YcwTIb9zpOOhoPsMm

          # disable gravatar profile images
          ;disable_gravatar = false

          # data source proxy whitelist (ip_or_domain:port separated by spaces)
          ;data_source_proxy_whitelist =

          # disable protection against brute force login attempts
          ;disable_brute_force_login_protection = false

          # set to true if you host Grafana behind HTTPS. default is false.
          ;cookie_secure = false

          # set cookie SameSite attribute. defaults to `lax`. can be set to "lax", "strict" and "none"
          ;cookie_samesite = lax

          # set to true if you want to allow browsers to render Grafana in a <frame>, <iframe>, <embed> or <object>. default is false.
          ;allow_embedding = false

          # Set to true if you want to enable http strict transport security (HSTS) response header.
          # This is only sent when HTTPS is enabled in this configuration.
          # HSTS tells browsers that the site should only be accessed using HTTPS.
          # The default version will change to true in the next minor release, 6.3.
          ;strict_transport_security = false

          # Sets how long a browser should cache HSTS. Only applied if strict_transport_security is enabled.
          ;strict_transport_security_max_age_seconds = 86400

          # Set to true if to enable HSTS preloading option. Only applied if strict_transport_security is enabled.
          ;strict_transport_security_preload = false

          # Set to true if to enable the HSTS includeSubDomains option. Only applied if strict_transport_security is enabled.
          ;strict_transport_security_subdomains = false

          # Set to true to enable the X-Content-Type-Options response header.
          # The X-Content-Type-Options response HTTP header is a marker used by the server to indicate that the MIME types advertised
          # in the Content-Type headers should not be changed and be followed. The default will change to true in the next minor release, 6.3.
          ;x_content_type_options = false

          # Set to true to enable the X-XSS-Protection header, which tells browsers to stop pages from loading
          # when they detect reflected cross-site scripting (XSS) attacks. The default will change to true in the next minor release, 6.3.
          ;x_xss_protection = false

          #################################### Snapshots ###########################
          [snapshots]
          # snapshot sharing options
          ;external_enabled = true
          ;external_snapshot_url = https://snapshots-origin.raintank.io
          ;external_snapshot_name = Publish to snapshot.raintank.io

          # Set to true to enable this Grafana instance act as an external snapshot server and allow unauthenticated requests for
          # creating and deleting snapshots.
          ;public_mode = false

          # remove expired snapshot
          ;snapshot_remove_expired = true

          #################################### Dashboards History ##################
          [dashboards]
          # Number dashboard versions to keep (per dashboard). Default: 20, Minimum: 1
          ;versions_to_keep = 20

          #################################### Users ###############################
          [users]
          # disable user signup / registration
          ;allow_sign_up = true

          # Allow non admin users to create organizations
          ;allow_org_create = true

          # Set to true to automatically assign new users to the default organization (id 1)
          ;auto_assign_org = true

          # Default role new users will be automatically assigned (if disabled above is set to true)
          ;auto_assign_org_role = Viewer

          # Background text for the user field on the login page
          ;login_hint = email or username
          ;password_hint = password

          # Default UI theme ("dark" or "light")
          ;default_theme = dark

          # External user management, these options affect the organization users view
          ;external_manage_link_url =
          ;external_manage_link_name =
          ;external_manage_info =

          # Viewers can edit/inspect dashboard settings in the browser. But not save the dashboard.
          ;viewers_can_edit = false

          # Editors can administrate dashboard, folders and teams they create
          ;editors_can_admin = false

          [auth]
          # Login cookie name
          ;login_cookie_name = grafana_session

          # The lifetime (days) an authenticated user can be inactive before being required to login at next visit. Default is 7 days,
          ;login_maximum_inactive_lifetime_days = 7

          # The maximum lifetime (days) an authenticated user can be logged in since login time before being required to login. Default is 30 days.
          ;login_maximum_lifetime_days = 30

          # How often should auth tokens be rotated for authenticated users when being active. The default is each 10 minutes.
          ;token_rotation_interval_minutes = 10

          # Set to true to disable (hide) the login form, useful if you use OAuth, defaults to false
          ;disable_login_form = false

          # Set to true to disable the signout link in the side menu. useful if you use auth.proxy, defaults to false
          ;disable_signout_menu = false

          # URL to redirect the user to after sign out
          ;signout_redirect_url =

          # Set to true to attempt login with OAuth automatically, skipping the login screen.
          # This setting is ignored if multiple OAuth providers are configured.
          ;oauth_auto_login = false

          #################################### Anonymous Auth ######################
          [auth.anonymous]
          # enable anonymous access
          ;enabled = false

          # specify organization name that should be used for unauthenticated users
          ;org_name = Main Org.

          # specify role for unauthenticated users
          ;org_role = Viewer

          #################################### Github Auth ##########################
          [auth.github]
          ;enabled = false
          ;allow_sign_up = true
          ;client_id = some_id
          ;client_secret = some_secret
          ;scopes = user:email,read:org
          ;auth_url = https://github.com/login/oauth/authorize
          ;token_url = https://github.com/login/oauth/access_token
          ;api_url = https://api.github.com/user
          ;team_ids =
          ;allowed_organizations =

          #################################### Google Auth ##########################
          [auth.google]
          ;enabled = false
          ;allow_sign_up = true
          ;client_id = some_client_id
          ;client_secret = some_client_secret
          ;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email
          ;auth_url = https://accounts.google.com/o/oauth2/auth
          ;token_url = https://accounts.google.com/o/oauth2/token
          ;api_url = https://www.googleapis.com/oauth2/v1/userinfo
          ;allowed_domains =

          #################################### Generic OAuth ##########################
          [auth.generic_oauth]
          ;enabled = false
          ;name = OAuth
          ;allow_sign_up = true
          ;client_id = some_id
          ;client_secret = some_secret
          ;scopes = user:email,read:org
          ;email_attribute_name = email:primary
          ;email_attribute_path =
          ;auth_url = https://foo.bar/login/oauth/authorize
          ;token_url = https://foo.bar/login/oauth/access_token
          ;api_url = https://foo.bar/user
          ;team_ids =
          ;allowed_organizations =
          ;tls_skip_verify_insecure = false
          ;tls_client_cert =
          ;tls_client_key =
          ;tls_client_ca =

          ; Set to true to enable sending client_id and client_secret via POST body instead of Basic authentication HTTP header
          ; This might be required if the OAuth provider is not RFC6749 compliant, only supporting credentials passed via POST payload
          ;send_client_credentials_via_post = false

          #################################### SAML Auth ###########################
          [auth.saml] # Enterprise only
          # Defaults to false. If true, the feature is enabled.
          ;enabled = false

          # Base64-encoded public X.509 certificate. Used to sign requests to the IdP
          ;certificate =

          # Path to the public X.509 certificate. Used to sign requests to the IdP
          ;certificate_path =

          # Base64-encoded private key. Used to decrypt assertions from the IdP
          ;private_key =

          ;# Path to the private key. Used to decrypt assertions from the IdP
          ;private_key_path =

          # Base64-encoded IdP SAML metadata XML. Used to verify and obtain binding locations from the IdP
          ;idp_metadata =

          # Path to the SAML metadata XML. Used to verify and obtain binding locations from the IdP
          ;idp_metadata_path =

          # URL to fetch SAML IdP metadata. Used to verify and obtain binding locations from the IdP
          ;idp_metadata_url =

          # Duration, since the IdP issued a response and the SP is allowed to process it. Defaults to 90 seconds.
          ;max_issue_delay = 90s

          # Duration, for how long the SP's metadata should be valid. Defaults to 48 hours.
          ;metadata_valid_duration = 48h

          # Friendly name or name of the attribute within the SAML assertion to use as the user's name
          ;assertion_attribute_name = displayName

          # Friendly name or name of the attribute within the SAML assertion to use as the user's login handle
          ;assertion_attribute_login = mail

          # Friendly name or name of the attribute within the SAML assertion to use as the user's email
          ;assertion_attribute_email = mail

          #################################### Grafana.com Auth ####################
          [auth.grafana_com]
          ;enabled = false
          ;allow_sign_up = true
          ;client_id = some_id
          ;client_secret = some_secret
          ;scopes = user:email
          ;allowed_organizations =

          #################################### Auth Proxy ##########################
          [auth.proxy]
          ;enabled = false
          ;header_name = X-WEBAUTH-USER
          ;header_property = username
          ;auto_sign_up = true
          ;ldap_sync_ttl = 60
          ;whitelist = 192.168.1.1, 192.168.2.1
          ;headers = Email:X-User-Email, Name:X-User-Name

          #################################### Basic Auth ##########################
          [auth.basic]
          ;enabled = true

          #################################### Auth LDAP ##########################
          [auth.ldap]
          ;enabled = false
          ;config_file = /etc/grafana/ldap.toml
          ;allow_sign_up = true

          # LDAP backround sync (Enterprise only)
          # At 1 am every day
          ;sync_cron = "0 0 1 * * *"
          ;active_sync_enabled = true

          #################################### SMTP / Emailing ##########################
          [smtp]
          ;enabled = false
          ;host = localhost:25
          ;user =
          # If the password contains # or ; you have to wrap it with trippel quotes. Ex """#password;"""
          ;password =
          ;cert_file =
          ;key_file =
          ;skip_verify = false
          ;from_address = admin@grafana.localhost
          ;from_name = Grafana
          # EHLO identity in SMTP dialog (defaults to instance_name)
          ;ehlo_identity = dashboard.example.com

          [emails]
          ;welcome_email_on_sign_up = false

          #################################### Logging ##########################
          [log]
          # Either "console", "file", "syslog". Default is console and  file
          # Use space to separate multiple modes, e.g. "console file"
          ;mode = console file

          # Either "debug", "info", "warn", "error", "critical", default is "info"
          ;level = info

          # optional settings to set different levels for specific loggers. Ex filters = sqlstore:debug
          ;filters =

          # For "console" mode only
          [log.console]
          ;level =

          # log line format, valid options are text, console and json
          ;format = console

          # For "file" mode only
          [log.file]
          ;level =

          # log line format, valid options are text, console and json
          ;format = text

          # This enables automated log rotate(switch of following options), default is true
          ;log_rotate = true

          # Max line number of single file, default is 1000000
          ;max_lines = 1000000

          # Max size shift of single file, default is 28 means 1 << 28, 256MB
          ;max_size_shift = 28

          # Segment log daily, default is true
          ;daily_rotate = true

          # Expired days of log file(delete after max days), default is 7
          ;max_days = 7

          [log.syslog]
          ;level =

          # log line format, valid options are text, console and json
          ;format = text

          # Syslog network type and address. This can be udp, tcp, or unix. If left blank, the default unix endpoints will be used.
          ;network =
          ;address =

          # Syslog facility. user, daemon and local0 through local7 are valid.
          ;facility =

          # Syslog tag. By default, the process' argv[0] is used.
          ;tag =

          #################################### Alerting ############################
          [alerting]
          # Disable alerting engine & UI features
          ;enabled = true
          # Makes it possible to turn off alert rule execution but alerting UI is visible
          ;execute_alerts = true

          # Default setting for new alert rules. Defaults to categorize error and timeouts as alerting. (alerting, keep_state)
          ;error_or_timeout = alerting

          # Default setting for how Grafana handles nodata or null values in alerting. (alerting, no_data, keep_state, ok)
          ;nodata_or_nullvalues = no_data

          # Alert notifications can include images, but rendering many images at the same time can overload the server
          # This limit will protect the server from render overloading and make sure notifications are sent out quickly
          ;concurrent_render_limit = 5


          # Default setting for alert calculation timeout. Default value is 30
          ;evaluation_timeout_seconds = 30

          # Default setting for alert notification timeout. Default value is 30
          ;notification_timeout_seconds = 30

          # Default setting for max attempts to sending alert notifications. Default value is 3
          ;max_attempts = 3

          #################################### Explore #############################
          [explore]
          # Enable the Explore section
          ;enabled = true

          #################################### Internal Grafana Metrics ##########################
          # Metrics available at HTTP API Url /metrics
          [metrics]
          # Disable / Enable internal metrics
          ;enabled           = true
          # Disable total stats (stat_totals_*) metrics to be generated
          ;disable_total_stats = false

          # Publish interval
          ;interval_seconds  = 10

          # Send internal metrics to Graphite
          [metrics.graphite]
          # Enable by setting the address setting (ex localhost:2003)
          ;address =
          ;prefix = prod.grafana.%(instance_name)s.

          #################################### Distributed tracing ############
          [tracing.jaeger]
          # Enable by setting the address sending traces to jaeger (ex localhost:6831)
          ;address = localhost:6831
          # Tag that will always be included in when creating new spans. ex (tag1:value1,tag2:value2)
          ;always_included_tag = tag1:value1
          # Type specifies the type of the sampler: const, probabilistic, rateLimiting, or remote
          ;sampler_type = const
          # jaeger samplerconfig param
          # for "const" sampler, 0 or 1 for always false/true respectively
          # for "probabilistic" sampler, a probability between 0 and 1
          # for "rateLimiting" sampler, the number of spans per second
          # for "remote" sampler, param is the same as for "probabilistic"
          # and indicates the initial sampling rate before the actual one
          # is received from the mothership
          ;sampler_param = 1
          # Whether or not to use Zipkin propagation (x-b3- HTTP headers).
          ;zipkin_propagation = false
          # Setting this to true disables shared RPC spans.
          # Not disabling is the most common setting when using Zipkin elsewhere in your infrastructure.
          ;disable_shared_zipkin_spans = false

          #################################### Grafana.com integration  ##########################
          # Url used to import dashboards directly from Grafana.com
          [grafana_com]
          ;url = https://grafana.com

          #################################### External image storage ##########################
          [external_image_storage]
          # Used for uploading images to public servers so they can be included in slack/email messages.
          # you can choose between (s3, webdav, gcs, azure_blob, local)
          ;provider =

          [external_image_storage.s3]
          ;bucket =
          ;region =
          ;path =
          ;access_key =
          ;secret_key =

          [external_image_storage.webdav]
          ;url =
          ;public_url =
          ;username =
          ;password =

          [external_image_storage.gcs]
          ;key_file =
          ;bucket =
          ;path =

          [external_image_storage.azure_blob]
          ;account_name =
          ;account_key =
          ;container_name =

          [external_image_storage.local]
          # does not require any configuration

          [rendering]
          # Options to configure external image rendering server like https://github.com/grafana/grafana-image-renderer
          ;server_url =
          ;callback_url =

          [enterprise]
          # Path to a valid Grafana Enterprise license.jwt file
          ;license_path =

          [panels]
          # If set to true Grafana will allow script tags in text panels. Not recommended as it enable XSS vulnerabilities.
          ;disable_sanitize_html = false

          [plugins]
          ;enable_alpha = false
          ;app_tls_skip_verify_insecure = false
      kind: ConfigMap
      metadata:
        name: grafana-config




    - apiVersion: v1
      data:
        datasources.yml: |-
          # config file version
          apiVersion: 1
          # list of datasources to insert/update depending
          # what's available in the database
          datasources:
            # <string, required> name of the datasource. Required
          - name: 'Prometheus'
            # <string, required> datasource type. Required
            type: prometheus
            # <string, required> access mode. proxy or direct (Server or Browser in the UI). Required
            access: proxy
            # <int> org id. will default to orgId 1 if not specified
            orgId: 1
            # <string> url
            url: http://prometheus:9090
            # <string> Deprecated, use secureJsonData.password
            isDefault: true
            # <map> fields that will be converted to json and stored in jsonData
            version: 1
            # <bool> allow users to edit datasources from the UI.
            editable: true
      kind: ConfigMap
      metadata:
        name: grafana-config-datasource

    - apiVersion: v1
      data:
        dashboard.yml: |-
          apiVersion: 1
          
          providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            options:
              path: /opt/grafana-dashboard
      kind: ConfigMap
      metadata:
        name: grafana-config-dashboard

    - apiVersion: v1
      data:
        prometheus.yml: |-
          # my global config
          global:
            scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
            evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
            # scrape_timeout is set to the global default (10s).

          # Alertmanager configuration
          alerting:
            alertmanagers:
            - static_configs:
              - targets:
                # - alertmanager:9093

          # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
          rule_files:
            # - "first_rules.yml"
            # - "second_rules.yml"

          # A scrape configuration containing exactly one endpoint to scrape:
          # Here it's Prometheus itself.
          scrape_configs:
            # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
            - job_name: 'prometheus'

              # metrics_path defaults to '/metrics'
              # scheme defaults to 'http'.

              static_configs:
              - targets: ['localhost:9090']

            - job_name: 'kafka_exporter'

              # metrics_path defaults to '/metrics'
              # scheme defaults to 'http'.

              static_configs:
              - targets: ['kafka-exporter:9308']
              
            - job_name: 'java'

              # metrics_path defaults to '/metrics'
              # scheme defaults to 'http'.

              static_configs:
              - targets: ['kafka-0.kafka.${NAMESPACE}.svc.cluster.local:9998']
              - targets: ['kafka-1.kafka.${NAMESPACE}.svc.cluster.local:9998']
              - targets: ['kafka-2.kafka.${NAMESPACE}.svc.cluster.local:9998']
              - targets: ['zookeeper-0.zookeeper.${NAMESPACE}.svc.cluster.local:9998']
              - targets: ['zookeeper-1.zookeeper.${NAMESPACE}.svc.cluster.local:9998']
              - targets: ['zookeeper-2.zookeeper.${NAMESPACE}.svc.cluster.local:9998']


      kind: ConfigMap
      metadata:
        name: prometheus-config        

  #####################

####prom grafana


### INICIO PROMETHEUS



      
      

    ## FIN PROMETHEUS

    - apiVersion: v1
      kind: DeploymentConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: prometheus
      spec:
        replicas: 1
        revisionHistoryLimit: 10
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: prometheus
        strategy:
          activeDeadlineSeconds: 21600
          recreateParams:
            timeoutSeconds: 600
          resources: {}
          type: Recreate
        template:
          metadata:
            labels:
              app: kafka-zookeeper-kafkamanager
              deploymentconfig: prometheus
          spec:
            containers:
              - image: "prometheus:latest"
                imagePullPolicy: Always
                name: prometheus
                ports:
                  - containerPort: 9090
                    protocol: TCP
                resources:
                  requests:
                    cpu: 500m
                    memory: 128Mi
                  limits:
                    cpu: 500m
                    memory: 512Mi
                livenessProbe:
                  exec:
                    command:
                    - sh
                    - "-c"
                    - |
                      nc -z -v -w5 127.0.0.1 9090
                      if [ "$?" == 0 ]; then
                      exit 0
                      else
                      exit 1
                      fi
                  initialDelaySeconds: 90
                  timeoutSeconds: 5
                readinessProbe:
                  exec:
                    command:
                    - sh
                    - "-c"
                    - |
                      nc -z -v -w5 127.0.0.1 9090
                      if [ "$?" == 0 ]; then
                      exit 0
                      else
                      exit 1
                      fi
                  initialDelaySeconds: 30
                  timeoutSeconds: 15                  
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                - name: prometheus
                  mountPath: /opt/prometheus-data
                  subPath: prometheus
                - name: prometheus-config
                  mountPath: /opt/prometheus-config               
            volumes:
              - name: prometheus
                persistentVolumeClaim:
                  claimName: grafana-prom-data
              - configMap:
                  name: prometheus-config
                name: prometheus-config                        
            dnsPolicy: ClusterFirst
            restartPolicy: Always
            schedulerName: default-scheduler
            terminationGracePeriodSeconds: 30
        test: false
        triggers:
          - type: ConfigChange
          - imageChangeParams:
              automatic: true
              containerNames:
                - prometheus
              from:
                kind: ImageStreamTag
                name: "prometheus:latest"
            type: ImageChange


    - apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: prometheus
      spec:
        ports:
          - name: http
            port: 9090
            protocol: TCP
            targetPort: 9090
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: prometheus
        sessionAffinity: None
        type: ClusterIP


    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
          build: prometheus
        name: prometheus
      spec:
        failedBuildsHistoryLimit: 5
        nodeSelector: null
        output:
          to:
            kind: ImageStreamTag
            name: 'prometheus:latest'
        postCommit: {}
        resources: {}
        runPolicy: Serial
        source:
          contextDir: prometheus
          git:
            ref: develop
            uri: 'https://github.com/mvilche/apache-kafka-openshift.git'
          type: Git
        strategy:
          dockerStrategy:
            noCache: true
            forcePull: true
            dockerfilePath: Dockerfile
          type: Docker
        successfulBuildsHistoryLimit: 5
        triggers:
          - type: ConfigChange

    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: prometheus
      spec: {}     


    - apiVersion: v1
      kind: DeploymentConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: grafana
      spec:
        replicas: 1
        revisionHistoryLimit: 10
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: grafana
        strategy:
          activeDeadlineSeconds: 21600
          recreateParams:
            timeoutSeconds: 600
          resources: {}
          type: Recreate
        template:
          metadata:
            labels:
              app: kafka-zookeeper-kafkamanager
              deploymentconfig: grafana
          spec:
            containers:
              - image: "grafana:latest"
                imagePullPolicy: Always
                name: grafana
                ports:
                  - containerPort: 3000
                    protocol: TCP
                resources:
                  requests:
                    cpu: 500m
                    memory: 128Mi
                  limits:
                    cpu: 1024m
                    memory: 1024Mi
                livenessProbe:
                  exec:
                    command:
                    - sh
                    - "-c"
                    - |
                      nc -z -v -w5 127.0.0.1 3000
                      if [ "$?" == 0 ]; then
                      exit 0
                      else
                      exit 1
                      fi
                  initialDelaySeconds: 90
                  timeoutSeconds: 5
                readinessProbe:
                  exec:
                    command:
                    - sh
                    - "-c"
                    - |
                      nc -z -v -w5 127.0.0.1 3000
                      if [ "$?" == 0 ]; then
                      exit 0
                      else
                      exit 1
                      fi
                  initialDelaySeconds: 30
                  timeoutSeconds: 15                  
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                - name: grafana
                  mountPath: /opt/grafana-data
                  subPath: grafana
                - name: grafana-config
                  mountPath: /opt/grafana-config
                - name: grafana-config-datasource
                  mountPath: /opt/grafana/conf/provisioning/datasources
                - name: grafana-config-dashboard
                  mountPath: /opt/grafana/conf/provisioning/dashboards                                      
            volumes:
              - name: grafana
                persistentVolumeClaim:
                  claimName: grafana-prom-data
              - configMap:
                  name: grafana-config
                name: grafana-config
              - configMap:
                  name: grafana-config-datasource
                name: grafana-config-datasource
              - configMap:
                  name: grafana-config-dashboard
                name: grafana-config-dashboard                                                   
            dnsPolicy: ClusterFirst
            restartPolicy: Always
            schedulerName: default-scheduler
            terminationGracePeriodSeconds: 30
        test: false
        triggers:
          - type: ConfigChange
          - imageChangeParams:
              automatic: true
              containerNames:
                - grafana
              from:
                kind: ImageStreamTag
                name: "grafana:latest"
            type: ImageChange


    - apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: grafana
      spec:
        ports:
          - name: http
            port: 3000
            protocol: TCP
            targetPort: 3000
        selector:
          app: kafka-zookeeper-kafkamanager
          deploymentconfig: grafana
        sessionAffinity: None
        type: ClusterIP


    - apiVersion: v1
      kind: Route
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: grafana-http
      spec:
        port:
          targetPort: http
        to:
          kind: Service
          name: grafana
          weight: 100
        wildcardPolicy: None
        
        
    - apiVersion: v1
      kind: Route
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: prometheus-http
      spec:
        port:
          targetPort: http
        to:
          kind: Service
          name: prometheus
          weight: 100
        wildcardPolicy: None          


    - apiVersion: v1
      kind: BuildConfig
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
          build: grafana
        name: grafana
      spec:
        failedBuildsHistoryLimit: 5
        nodeSelector: null
        output:
          to:
            kind: ImageStreamTag
            name: 'grafana:latest'
        postCommit: {}
        resources: {}
        runPolicy: Serial
        source:
          contextDir: grafana
          git:
            ref: develop
            uri: 'https://github.com/mvilche/apache-kafka-openshift.git'
          type: Git
        strategy:
          dockerStrategy:
            noCache: true
            forcePull: true
            dockerfilePath: Dockerfile
          type: Docker
        successfulBuildsHistoryLimit: 5
        triggers:
          - type: ConfigChange

    - apiVersion: v1
      kind: ImageStream
      metadata:
        labels:
          app: kafka-zookeeper-kafkamanager
        name: grafana
      spec: {}

    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        finalizers:
        - kubernetes.io/pvc-protection
        name: grafana-prom-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi    


#######
  ################PARAMETROS
    parameters:
      - name: NAMESPACE
        displayName: Nombre del proyecto donde esta desplegando el template. Respete maysculas y minÃºsculas
        value: ''
        required: true

  ###############
